{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d20b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byc324/Desktop/24Fall/25Fall/communication/assignment1/.venv/lib/python3.12/site-packages/convokit/coordination/coordination.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, re, json, shutil, datetime, zipfile\n",
    "from pathlib import Path\n",
    "from convokit import Corpus, Speaker, Utterance, Conversation\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155d4cf",
   "metadata": {},
   "source": [
    "### Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e15fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3448\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=\"./QuestionEvasion-convokit\")\n",
    "print(len(list(corpus.iter_conversations())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cd43d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 5\n",
      "Number of Utterances: 6896\n",
      "Number of Conversations: 3448\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b64edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation('id': 'Q_3036', 'utterances': ['Q_3036', 'A_3036'], 'meta': {'title': \"The President's News Conference\", 'date': 'December 04, 2007', 'url': 'https://www.presidency.ucsb.edu/documents/the-presidents-news-conference-1132', 'president': 'George W. Bush', 'question_order': 7})\n"
     ]
    }
   ],
   "source": [
    "# conversation\n",
    "convo = corpus.random_conversation()\n",
    "print(convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8aca4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you respond to the accusation that the United States is containing China while pushing for diplomatic talks?\n"
     ]
    }
   ],
   "source": [
    "# utterance\n",
    "for utt in corpus.iter_utterances():\n",
    "    print(utt.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d43a63",
   "metadata": {},
   "source": [
    "## Hypothesis: Whether adding the question to the answer as context improves the accuracy of the classifier about whether the answer is ambiguous or not (i.e. ambivalent, clear, or clear-nonreply)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf73cc4",
   "metadata": {},
   "source": [
    "### Pre-process Labels According to the Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0469e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer labels: Counter({'Explicit': 1052, 'Dodging': 706, 'Implicit': 488, 'General': 386, 'Deflection': 381, 'Declining to answer': 145, 'Claims ignorance': 119, 'Clarification': 92, 'Partial/half-answer': 79})\n"
     ]
    }
   ],
   "source": [
    "# Labels are only included in the answer utterances\n",
    "a_labels = [u.meta.get(\"label\") for u in corpus.iter_utterances() if u.meta.get(\"type\")==\"answer\"]\n",
    "print(\"Answer labels:\", Counter(a_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9771c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ambivalent-reply': 2040, 'clear-reply': 1052, 'clear-nonreply': 356})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def norm(s):\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "CLEAR_REPLY = {\"Explicit\"}\n",
    "AMBIV_REPLY = {\"Implicit\", \"Dodging\", \"General\", \"Deflection\", \"Partial/half-answer\"}\n",
    "CLEAR_NON   = {\"Declining to answer\", \"Claims ignorance\", \"Clarification\"}\n",
    "\n",
    "def map_label(lbl):\n",
    "    if lbl in CLEAR_REPLY:\n",
    "        return \"clear-reply\"\n",
    "    elif lbl in AMBIV_REPLY:\n",
    "        return \"ambivalent-reply\"\n",
    "    elif lbl in CLEAR_NON:\n",
    "        return \"clear-nonreply\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for u in corpus.iter_utterances():\n",
    "    if u.meta.get(\"type\") == \"answer\" and \"label\" in u.meta:\n",
    "        u.meta[\"coarse_label\"] = map_label(u.meta[\"label\"])\n",
    "\n",
    "coarse_counts = Counter(\n",
    "    u.meta.get(\"coarse_label\")\n",
    "    for u in corpus.iter_utterances()\n",
    "    if u.meta.get(\"type\") == \"answer\"\n",
    ")\n",
    "print(coarse_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd27f44",
   "metadata": {},
   "source": [
    "### Condition 1: Answer alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3314ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing default unigram CountVectorizer...Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  bag-of-words vectors\n",
    "from convokit import BoWTransformer\n",
    "bow_transformer = BoWTransformer(obj_type=\"utterance\", vector_name=\"bow_A\")\n",
    "bow_transformer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9353ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized default classification model (standard scaled logistic regression).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byc324/Desktop/24Fall/25Fall/communication/assignment1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "/Users/byc324/Desktop/24Fall/25Fall/communication/assignment1/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier\n",
    "from convokit import VectorClassifier\n",
    "# selector: as only answer utterances have labels\n",
    "is_ans_labeled = lambda u: u.meta.get(\"type\")==\"answer\" and \"coarse_label\" in u.meta\n",
    "\n",
    "clf_A = VectorClassifier(\n",
    "    obj_type=\"utterance\",\n",
    "    vector_name=\"bow_A\",\n",
    "    labeller=lambda u: u.meta.get(\"coarse_label\") if is_ans_labeled(u) else None\n",
    ")\n",
    "\n",
    "clf_A.fit_transform(corpus, selector=is_ans_labeled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7250f33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A_3229</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_2722</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>0.999686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_2723</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>0.999686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_2676</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>0.999330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1966</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>0.999114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3443</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3444</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3445</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3446</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3447</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            prediction  pred_score\n",
       "id                                \n",
       "A_3229  clear-nonreply    1.000000\n",
       "A_2722  clear-nonreply    0.999686\n",
       "A_2723  clear-nonreply    0.999686\n",
       "A_2676  clear-nonreply    0.999330\n",
       "A_1966  clear-nonreply    0.999114\n",
       "...                ...         ...\n",
       "Q_3443            None         NaN\n",
       "Q_3444            None         NaN\n",
       "Q_3445            None         NaN\n",
       "Q_3446            None         NaN\n",
       "Q_3447            None         NaN\n",
       "\n",
       "[6896 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "clf_A.summarize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa2aab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority label: ambivalent-reply\n",
      "Base accuracy: 0.5916473317865429\n"
     ]
    }
   ],
   "source": [
    "# Base Accuracy\n",
    "y_true, _ = clf_A.get_y_true_pred(corpus, selector=is_ans_labeled)  \n",
    "cnt = Counter(y_true)\n",
    "maj_label, maj_count = max(cnt.items(), key=lambda x: x[1])\n",
    "base_acc = maj_count / sum(cnt.values())\n",
    "print(\"Majority label:\", maj_label)\n",
    "print(\"Base accuracy:\", base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a25b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8474477958236659)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_A.accuracy(corpus, selector=is_ans_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe1706a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "ambivalent-reply       0.86      0.90      0.88      2040\n",
      "  clear-nonreply       0.88      0.79      0.83       356\n",
      "     clear-reply       0.81      0.76      0.78      1052\n",
      "\n",
      "        accuracy                           0.85      3448\n",
      "       macro avg       0.85      0.82      0.83      3448\n",
      "    weighted avg       0.85      0.85      0.85      3448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clf_A.classification_report(corpus, selector=is_ans_labeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa623c",
   "metadata": {},
   "source": [
    "Analysis (Condition 1: Pure Answer, No Train/Test Split)\n",
    "\n",
    "-- Using only the answer text as features, our classifier achieves an accuracy of 0.85, which is notably higher than the base accuracy of 0.59. Precision and recall are relatively balanced across the three classes, though performance is strongest on ambivalent-reply and weaker on clear-reply. This suggests that answer content alone carries useful signals for classification.\n",
    "\n",
    "-- As a next step, I plan to explore a Condition 2 setting where the question text is concatenated with the answer (Q + A). Adding the question as context may provide richer semantic cues, potentially improving disambiguation between clear-reply and clear-nonreply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d6808",
   "metadata": {},
   "source": [
    "## Condition 2: Question + Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34eb9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping question text to answer text\n",
    "aq_text = {}\n",
    "for conv in corpus.iter_conversations():\n",
    "    for uid in conv.get_utterance_ids(): \n",
    "        if uid.startswith(\"A_\"):\n",
    "            qid = \"Q_\" + uid.split(\"_\", 1)[1]\n",
    "            q = corpus.get_utterance(qid)\n",
    "            aq_text[uid] = (q.text if q and q.text else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af67cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing default unigram CountVectorizer...Done.\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words vectors processing: add prefix Q and A to the tokens\n",
    "\n",
    "re_tok = re.compile(r\"\\w+\")\n",
    "\n",
    "def qa_prefixed(u):\n",
    "    if u.meta.get(\"type\") != \"answer\":  \n",
    "        return None   # only generate vectors for answer utterances\n",
    "    \n",
    "    # find question text\n",
    "    q_text = aq_text.get(u.id, \"\") or \"\"\n",
    "    a_text = u.text or \"\"\n",
    "\n",
    "    # add prefix to distinguish question and answer tokens\n",
    "    q_tokens = [f\"Q_{m.group(0).lower()}\" for m in re_tok.finditer(q_text)]\n",
    "    a_tokens = [f\"A_{m.group(0).lower()}\" for m in re_tok.finditer(a_text)]\n",
    "\n",
    "    # concatenate question and answer tokens\n",
    "    return \" \".join(q_tokens + a_tokens)\n",
    "\n",
    "bow_QA = BoWTransformer(\n",
    "    obj_type=\"utterance\",\n",
    "    vector_name=\"bow_QA_prefixed\",\n",
    "    text_func=qa_prefixed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6a15341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_QA.fit_transform(corpus, selector=is_ans_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ddc51b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bow_A', 'bow_QA_prefixed'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d975d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance ID: A_2310\n",
      "Original answer text: Okay. Good. I think we've just answered the question.\n",
      "QA text_func: Q_asking Q_for Q_the Q_president Q_s Q_understanding Q_of Q_people Q_s Q_perception Q_of Q_legitimizing Q_a Q_regime Q_and Q_oppressing Q_its Q_people Q_by Q_meeting Q_and Q_shaking Q_hands Q_with Q_the Q_leader Q_of Q_north Q_korea A_okay A_good A_i A_think A_we A_ve A_just A_answered A_the A_question\n"
     ]
    }
   ],
   "source": [
    "# check the vector of a random answer\n",
    "vocab = bow_QA.vectorizer.get_feature_names_out()\n",
    "import random\n",
    "u = random.choice([utt for utt in corpus.iter_utterances() if utt.meta.get(\"type\")==\"answer\"])\n",
    "vec = u.get_vector(\"bow_QA_prefixed\")\n",
    "\n",
    "print(\"Utterance ID:\", u.id)\n",
    "print(\"Original answer text:\", u.text)\n",
    "print(\"QA text_func:\", qa_prefixed(u))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4dba341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized default classification model (standard scaled logistic regression).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byc324/Desktop/24Fall/25Fall/communication/assignment1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier\n",
    "clf_QA = VectorClassifier(\n",
    "    obj_type=\"utterance\",\n",
    "    vector_name=\"bow_QA_prefixed\",\n",
    "    labeller=lambda u: u.meta.get(\"coarse_label\") if is_ans_labeled(u) else None\n",
    ")\n",
    "clf_QA.fit_transform(corpus, selector=is_ans_labeled)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe1a086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A_1967</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1966</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_1964</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_2063</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_825</th>\n",
       "      <td>clear-nonreply</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3443</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3444</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3445</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3446</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q_3447</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            prediction  pred_score\n",
       "id                                \n",
       "A_1967  clear-nonreply    1.000000\n",
       "A_1966  clear-nonreply    1.000000\n",
       "A_1964  clear-nonreply    1.000000\n",
       "A_2063  clear-nonreply    1.000000\n",
       "A_825   clear-nonreply    0.999999\n",
       "...                ...         ...\n",
       "Q_3443            None         NaN\n",
       "Q_3444            None         NaN\n",
       "Q_3445            None         NaN\n",
       "Q_3446            None         NaN\n",
       "Q_3447            None         NaN\n",
       "\n",
       "[6896 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_QA.summarize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "912d7440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9904292343387471)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_QA.accuracy(corpus, selector=is_ans_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2daf2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "ambivalent-reply       0.99      0.99      0.99      2040\n",
      "  clear-nonreply       1.00      0.98      0.99       356\n",
      "     clear-reply       0.99      0.99      0.99      1052\n",
      "\n",
      "        accuracy                           0.99      3448\n",
      "       macro avg       0.99      0.99      0.99      3448\n",
      "    weighted avg       0.99      0.99      0.99      3448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clf_QA.classification_report(corpus, selector=is_ans_labeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82073291",
   "metadata": {},
   "source": [
    "Analysis (Condition 2: Question + Answer, No Train/Test Split)\n",
    "\n",
    "Using concatenated question–answer representations, the classifier achieved extremely high performance (0.99 precision/recall/F1 across all three classes). This suggests that incorporating the question as context can substantially boost discriminative signal compared to using the answer alone. However, the near-perfect scores are also indicative of potential overfitting, since evaluation was conducted on the same data used for training. To more rigorously assess generalizability, I therefore introduce Condition 3, where I split the dataset into separate train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c3828",
   "metadata": {},
   "source": [
    "## Condition 3: Question + Answer & Split Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc4c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling and creating new metadata indicating train and test; still, only answers have labels\n",
    "answers = [u for u in corpus.iter_utterances() \n",
    "           if u.meta.get(\"type\")==\"answer\" and \"coarse_label\" in u.meta]\n",
    "y = [u.meta[\"coarse_label\"] for u in answers]\n",
    "train_utts, test_utts = train_test_split(answers, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "for u in train_utts:\n",
    "    u.meta[\"split\"] = \"train\"\n",
    "for u in test_utts:\n",
    "    u.meta[\"split\"] = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67b308cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectors\n",
    "is_answer  = lambda u: u.meta.get(\"type\") == \"answer\"\n",
    "is_train_u = lambda u: is_answer(u) and u.meta.get(\"split\") == \"train\"\n",
    "is_test_u  = lambda u: is_answer(u) and u.meta.get(\"split\") == \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a68c1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing default unigram CountVectorizer...Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag-of-words vectors processing: add prefix Q and A to the tokens\n",
    "bow_QA_split = BoWTransformer(\n",
    "    obj_type=\"utterance\",\n",
    "    vector_name=\"bow_QA_prefixed_split\",  \n",
    "    text_func=qa_prefixed                  \n",
    ")\n",
    "\n",
    "# learn vocabulary only in train dataset\n",
    "bow_QA_split.fit(corpus, selector=is_train_u)\n",
    "\n",
    "# vectorize both train and test dataset\n",
    "bow_QA_split.transform(corpus, selector=is_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdad3873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized default classification model (standard scaled logistic regression).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/byc324/Desktop/24Fall/25Fall/communication/assignment1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x14fbe6b70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier\n",
    "clf_QA_split = VectorClassifier(\n",
    "    obj_type=\"utterance\",\n",
    "    vector_name=\"bow_QA_prefixed_split\",\n",
    "    labeller=lambda u: u.meta.get(\"coarse_label\") if is_answer(u) else None\n",
    ")\n",
    "\n",
    "# only fit on train dataset\n",
    "clf_QA_split.fit(corpus, selector=is_train_u)\n",
    "\n",
    "# create prediction on both train and test dataset\n",
    "clf_QA_split.transform(corpus, selector=is_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4914bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.994198694706309\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "ambivalent-reply       0.99      1.00      1.00      1632\n",
      "  clear-nonreply       1.00      0.99      0.99       285\n",
      "     clear-reply       0.99      0.99      0.99       841\n",
      "\n",
      "        accuracy                           0.99      2758\n",
      "       macro avg       1.00      0.99      0.99      2758\n",
      "    weighted avg       0.99      0.99      0.99      2758\n",
      "\n",
      "Test accuracy: 0.5507246376811594\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "ambivalent-reply       0.65      0.65      0.65       408\n",
      "  clear-nonreply       0.44      0.38      0.41        71\n",
      "     clear-reply       0.41      0.42      0.41       211\n",
      "\n",
      "        accuracy                           0.55       690\n",
      "       macro avg       0.50      0.48      0.49       690\n",
      "    weighted avg       0.55      0.55      0.55       690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Train accuracy:\", clf_QA_split.accuracy(corpus, selector=is_train_u))\n",
    "print(clf_QA_split.classification_report(corpus, selector=is_train_u))\n",
    "\n",
    "print(\"Test accuracy:\", clf_QA_split.accuracy(corpus, selector=is_test_u))\n",
    "print(clf_QA_split.classification_report(corpus, selector=is_test_u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737deff",
   "metadata": {},
   "source": [
    "Analysis (Condition 3:  Question + Answer, Train/Test Split)\n",
    "\n",
    "When evaluated under a proper train–test split, the classifier shows extremely high performance on the training set (accuracy 0.99) but drops sharply on the test set (accuracy 0.55), even underperforming the base accuracy. This indicates severe overfitting: the model memorizes training data but fails to generalize to unseen examples. The gap demonstrates the limitation of the bag-of-words approach and suggests that richer contextual representations or more powerful models (e.g., LLMs) are needed to capture the nuances of question–answer interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
